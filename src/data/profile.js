export const profile = {
  name: "Dr. Alex Morgan",
  title: "AI Research Engineer",
  avatar:
    "https://ui-avatars.com/api/?name=Alex+Morgan&size=200&background=2C5282&color=fff&bold=true",
  bio: `
    <p>Research engineer specializing in machine learning systems, natural language processing, and large language models. Currently focused on developing efficient training methodologies and deployment strategies for production ML systems.</p>
    
    <p>Extensive experience in applied AI research with published work on neural architecture optimization, distributed training systems, and transformer-based models. Primary research interests include model compression, federated learning, and interpretable AI.</p>
    
    <p>Active contributor to open-source ML frameworks and author of peer-reviewed publications in top-tier conferences. Passionate about bridging the gap between research and production systems.</p>
  `,
  currentFocus: [
    {
      area: "Efficient LLM Training",
      description:
        "Developing novel optimization techniques for reducing computational costs in large language model training, focusing on gradient checkpointing and mixed-precision strategies.",
    },
    {
      area: "Federated Learning Systems",
      description:
        "Designing privacy-preserving distributed learning architectures for cross-organizational collaboration while maintaining data sovereignty.",
    },
  ],
  researchPhilosophy: `I believe in research that bridges theory and practice. Every algorithmic innovation should be validated in real-world systems, and every production challenge should inspire theoretical investigation. My work emphasizes reproducibility, rigorous empirical evaluation, and open science principles.`,
  researchInterests: [
    "Model Compression & Quantization",
    "Federated & Privacy-Preserving ML",
    "Neural Architecture Search",
    "Distributed Training Systems",
    "Interpretable AI & Explainability",
    "Transfer Learning & Few-Shot Learning",
    "Multi-Modal Learning",
    "Efficient Transformers",
  ],
  technicalSkills: {
    frameworks: [
      "PyTorch",
      "TensorFlow",
      "JAX",
      "Hugging Face",
      "Ray",
      "MLflow",
    ],
    languages: ["Python", "C++", "CUDA", "Rust", "Julia", "SQL"],
    domains: [
      "Natural Language Processing",
      "Computer Vision",
      "Reinforcement Learning",
      "Time Series Analysis",
    ],
    infrastructure: [
      "Docker",
      "Kubernetes",
      "AWS",
      "GCP",
      "Distributed Computing",
      "MLOps Pipelines",
    ],
  },
  researchMetrics: {
    publications: "15+",
    citations: "1,200+",
    hIndex: "12",
    openSourceProjects: "8",
  },
  speaking: [
    "Guest lectures on ML systems at various institutions",
    "Regular speaker at AI/ML conferences and meetups",
    "Organizer of local machine learning reading groups",
    "Tutorial presenter on distributed training techniques",
  ],
  certifications: [
    "AWS Certified Machine Learning - Specialty",
    "Deep Learning Specialization",
    "TensorFlow Developer Certificate",
    "Advanced NLP with Transformers",
  ],
  contact: {
    email: "tatra.labs@gmail.com",
    github: "https://github.com/tatra-labs",
    twitter: "https://twitter.com/TatraLabs",
    medium: "https://medium.com/@tatra.labs",
  },
};
